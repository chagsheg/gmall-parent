package com.atguigu.realtime.gmalllogger.controller;import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.JSONObject;import lombok.extern.slf4j.Slf4j;import org.apache.kafka.clients.producer.ProducerRecord;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.kafka.core.KafkaTemplate;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PostMapping;import org.springframework.web.bind.annotation.RequestBody;import org.springframework.web.bind.annotation.RestController;@RestController@Slf4jpublic class LoggerController {    @PostMapping("/applog")/*kafka的分区如何定呢？因为我们使用的是直连模式，所以 kafka的分区是和 spark-streaming的分区是对应的，即kafka的分区应该使用spark的并行度决定  ==》 kafka的分区由下游的消费者来定// Spark消费上游kafka数据时候，是强制的一对一消费，也就是上游4分区，对应Spark的4个core（即4个executors） */    public String doLog(@RequestBody String logString){     System.out.println(logString);        // 1.日志落盘 ，这里用日志框架写，用lockback写   saveToDisk(logString);  sendToKafka(logString);        return "ok";    }    private void saveToDisk(String logString) {        log.info(logString);    }    @Autowired    KafkaTemplate kafkaTemplate;    private void sendToKafka(String logString) {        JSONObject obj = JSON.parseObject(logString);        /* {"common":{"ar":"110000","ba":"iPhone","ch":"Appstore","md":"iPhone X","mid":"mid_22","os":"iOS 13.3.1","uid":"182","vc":"v2.1.134"},"page":{"during_time":9157,"item":"5,6,9","item_type":"sku_ids","last_page_id":"trade","page_id":"payment"},"ts":1609236131819}{"common":{"ar":"110000","ba":"Huawei","ch":"oppo","md":"Huawei P30","mid":"mid_31","os":"Android 11.0","uid":"487","vc":"v2.1.134"},"start":{"entry":"icon","loading_time":16331,"open_ad_id":5,"open_ad_ms":5463,"open_ad_skip_ms":5065},"ts":1609236066000}         */        // 启动日志和事件日志进入到不同的 topic        if (obj.getString("start") != null && obj.getString("start").length() > 0) {            kafkaTemplate.send("gmall_startup_topic",logString);        } else {            kafkaTemplate.send("gmall_event_topic", logString);        }    }}